{
 "metadata": {
  "name": "",
  "signature": "sha256:4f9ef318a2286486cbf3952be8ffec3bbcf81a1ffd787ea5767cd829f5f4f705"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Impl\u00e9mentation des Losses\n",
      "======\n",
      "\n",
      "Nous allons nous int\u00e9resser \u00e0 l'impl\u00e9mentation \"g\u00e9n\u00e9rique\" d'un co\u00fbt (loss) de pr\u00e9diction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from TP2_perceptron import createGaussianDataset, LabeledSet, plot2DSet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class Loss:\n",
      "    \n",
      "    #Calcule la valeur du loss \u00e9tant donn\u00e9es les valeurs pr\u00e9dites et d\u00e9sir\u00e9es\n",
      "    def getLossValue(self,predicted_output,desired_output):\n",
      "        pass\n",
      "    \n",
      "    #Calcule le gradient (pour chaque celllule d'entr\u00e9e) du co\u00fbt\n",
      "    def backward(self, predicted_output,desired_output):\n",
      "        pass "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set=createGaussianDataset(1,1,1,-2,-2,1,10)\n",
      "#plot2DSet(set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Impl\u00e9menter le co\u00fbt des moindres carr\u00e9s selon cette sp\u00e9cification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class SquareLoss(Loss):\n",
      "    \n",
      "    def getLossValue(self,predicted_output,desired_output):\n",
      "        y_pred = predicted_output\n",
      "        y = desired_output\n",
      "        \n",
      "        return sum( np.square(y_pred-y) )\n",
      "          \n",
      "    def backward(self, predicted_output,desired_output):\n",
      "        y_pred = predicted_output\n",
      "        y = desired_output\n",
      "        delta_out = y_pred*2*(y_pred-y)\n",
      "        return delta_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Immpl\u00e9menter le ''hinge loss''"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class HingeLoss(Loss): #L = max(0,-y_pred*y)\n",
      "\n",
      "    def getLossValue(self,predicted_output,desired_output):\n",
      "        y_pred = predicted_output\n",
      "        y = desired_output\n",
      "        loss = sum(np.where(y_pred*y >= 0,0,-y_pred*y))\n",
      "        return loss\n",
      "          \n",
      "    def backward(self, predicted_output,desired_output):\n",
      "        y_pred = predicted_output\n",
      "        y = desired_output\n",
      "        delta_out = y_pred*np.where(y_pred*y >= 0,0,-y_pred)\n",
      "        return delta_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = np.random.randint(-10,10,(10,1))\n",
      "y_pred = np.where(y_pred >=0,1,-1)\n",
      "\n",
      "y =np.random.randint(-10,10,(10,1))\n",
      "y = np.where(y >=0,1,-1)\n",
      "print \"y_pred | y \\n\", np.hstack((y_pred,y))\n",
      "\n",
      "Loss_square = SquareLoss();\n",
      "Loss_hinge = HingeLoss();\n",
      "print \"Loss_square | Loss_hinge:\\n\",Loss_square.getLossValue(y_pred,y), Loss_hinge.getLossValue(y_pred,y)\n",
      "print \"square.backward | hinge.backward : \\n\", np.hstack((Loss_square.backward(y_pred,y) , Loss_hinge.backward(y_pred,y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y_pred | y \n",
        "[[ 1 -1]\n",
        " [-1 -1]\n",
        " [ 1 -1]\n",
        " [ 1  1]\n",
        " [-1 -1]\n",
        " [-1  1]\n",
        " [ 1 -1]\n",
        " [-1  1]\n",
        " [-1  1]\n",
        " [ 1 -1]]\n",
        "Loss_square | Loss_hinge:\n",
        "[28] [7]\n",
        "square.backward | hinge.backward : \n",
        "[[ 4 -1]\n",
        " [ 0  0]\n",
        " [ 4 -1]\n",
        " [ 0  0]\n",
        " [ 0  0]\n",
        " [ 4 -1]\n",
        " [ 4 -1]\n",
        " [ 4 -1]\n",
        " [ 4 -1]\n",
        " [ 4 -1]]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Impl\u00e9mentation des Modules\n",
      "======\n",
      "\n",
      "Nous allons maintenant impl\u00e9menter quelques modules de base"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class Module:\n",
      "    \n",
      "    #Permet le calcul de la sortie du module\n",
      "    def forward(self,input):\n",
      "        pass\n",
      "    \n",
      "    #Permet le calcul du gradient des cellules d'entr\u00e9e\n",
      "    def backward_delta(self,input,delta_module_suivant):\n",
      "        pass\n",
      "    \n",
      "    #Permet d'initialiser le gradient du module\n",
      "    def init_gradient(self):\n",
      "        pass\n",
      "    \n",
      "    #Permet la mise \u00e0 jour des parma\u00e8tres du module avcec la valeur courante di gradient\n",
      "    def update_parameters(self,gradient_step):\n",
      "        pass\n",
      "    \n",
      "    #Permet de mettre \u00e0 jour la valeur courante du gradient par addition\n",
      "    def backward_update_gradient(self,input,delta_module_suivant):\n",
      "        pass\n",
      "    \n",
      "    #Permet de faire les deux backwar simultan\u00e9ment\n",
      "    def backward(self,input,delta_module_suivant):\n",
      "        self.backward_update_gradient(input,delta_module_suivant)\n",
      "        return self.backward_delte(input,delta_module_suivant)\n",
      "\n",
      "    #Retourne les param\u00e8tres du module\n",
      "    def get_parameters(self):\n",
      "        pass\n",
      "    \n",
      "    #Initialize al\u00e9atoirement les param\u00e8tres du module\n",
      "    def randomize_parameters(self, variance):\n",
      "        pass\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous allons commencer par impl\u00e9menter le module lineaire classique (sans biais)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class LinearModule(Module):\n",
      "    \n",
      "    def __init__(self,input_dimension,output_dimension):\n",
      "        self.input_dimension = input_dimension\n",
      "        self.output_dimension = output_dimension \n",
      "        self.theta=np.ones((self.input_dimension,self.output_dimension))\n",
      "    \n",
      "    #Permet le calcul de la sortie du module\n",
      "    def forward(self,input):\n",
      "        out = np.dot(input,self.theta)\n",
      "        return out\n",
      "    \n",
      "    #Permet le calcul du gradient des cellules d'entr\u00e9e\n",
      "    def backward_delta(self,input,delta_module_suivant):\n",
      "        theta_i_j = input\n",
      "        delta_out = delta_module_suivant\n",
      "        \n",
      "        delta_in = np.dot(theta_i_j,delta_out)\n",
      "        \n",
      "        return delta_in\n",
      "    \n",
      "    \n",
      "    #Permet d'initialiser le gradient du module\n",
      "    def init_gradient(self):\n",
      "        pass\n",
      "    \n",
      "    #Permet la mise \u00e0 jour des parma\u00e8tres du module avcec la valeur courante di gradient\n",
      "    def update_parameters(self,gradient_step):\n",
      "        pass\n",
      "    \n",
      "    #Permet de mettre \u00e0 jour la valeur courante du gradient par addition\n",
      "    def backward_update_gradient(self,input,delta_module_suivant):\n",
      "        input.shape = (input.shape[0],1)\n",
      "        delta_module_suivant.shape = (1,delta_module_suivant.shape[0])\n",
      "        \n",
      "        return np.dot(input,delta_module_suivant)\n",
      "\n",
      "    \n",
      "\n",
      "    #Retourne les param\u00e8tres du module\n",
      "    def get_parameters(self):\n",
      "        return self.theta\n",
      "    \n",
      "    #Initialize al\u00e9atoirement les param\u00e8tres du module\n",
      "    def randomize_parameters(self, variance):\n",
      "        self.theta=np.ones((self.input_dimension,self.output_dimension))\n",
      "        return self.theta\n",
      "    \n",
      "    \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.random.randint(1,10,(5)) ; print \"y\", y\n",
      "\n",
      "\n",
      "module = LinearModule(2,y.shape[0]) ; print \"module.get_parameters()\\n\", module.get_parameters()\n",
      "hinge = HingeLoss()\n",
      "\n",
      "\n",
      "y_pred = module.forward(set.x[0,:]) ; print \"y_pred\\n\", y_pred\n",
      "lil_delta = hinge.backward(y_pred,y) ; print \"lil_delta\\n\", lil_delta\n",
      "\n",
      "backward = module.backward_delta(y_pred,lil_delta) ; \n",
      "print \"backward_delta \\n\"; print backward\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y [9 9 5 5 4]\n",
        "module.get_parameters()\n",
        "[[ 1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.]]\n",
        "y_pred\n",
        "[ 0.23768242  0.23768242  0.23768242  0.23768242  0.23768242]\n",
        "lil_delta\n",
        "[ 0.  0.  0.  0.  0.]\n",
        "backward_delta \n",
        "\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 5)\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test du Syst\u00e8me\n",
      "======\n",
      "\n",
      "Nous allons maintenant tester notre syst\u00e8me (Module Lin\u00e9aire-> Square Loss) sur un jeu de donn\u00e9es classiques (jeu en 2D du TP pr\u00e9c\u00e9dent). Est-ce que ca marche ? \n",
      "Essayez maintenant avec un hinge loss. Est-ce que ca marche?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Modules Additionnels\n",
      "======\n",
      "\n",
      "Nous allons impl\u00e9menter les modules suivants:\n",
      "* Module Tangente Hyperbolic\n",
      "* Module S\u00e9quentiel\n",
      "\n",
      "Nous pouvons maintenant faire des r\u00e9seaux de neurones ! "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}