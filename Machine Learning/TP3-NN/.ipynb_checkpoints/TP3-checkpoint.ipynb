{
 "metadata": {
  "name": "",
  "signature": "sha256:1d8ee37c51244449383e7977d6bce634bff98b76c3a7a769cdef6a6727b94539"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Impl\u00e9mentation des Losses\n",
      "======\n",
      "\n",
      "Nous allons nous int\u00e9resser \u00e0 l'impl\u00e9mentation \"g\u00e9n\u00e9rique\" d'un co\u00fbt (loss) de pr\u00e9diction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from tools import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Perceptron"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data=createGaussianDataset(-1,-1,0.5,2,2,0.5,100) ;\n",
      "#print np.hstack((set.x,set.y))\n",
      "#data.x = kernel_poly(data.x)\n",
      "label = data.y\n",
      "#plot2DSet(data)\n",
      "\n",
      "hinge = HingeLoss()\n",
      "square = SquareLoss()\n",
      "module = LinearModule(data.x.shape[1],1)\n",
      "\n",
      "module.randomize_parameters(10) ; #print module.get_parameters()\n",
      "\n",
      "for i in range(2000):\n",
      "    \n",
      "    module.init_gradient()\n",
      "    \n",
      "    t =np.random.choice(data.x.shape[0])\n",
      "    x = data.x[t,:]\n",
      "    y = label[t]\n",
      "    \n",
      "    y_pred = module.forward(x) ;#print \"y_pred | y\\n\", np.hstack((y_pred,y))\n",
      "    \n",
      "    delta_out = hinge.backward(y_pred,y) ; #print \"delta_out\", delta_out\n",
      "   \n",
      "    module.backward_update_gradient(x,delta_out)\n",
      "    \n",
      "    module.update_parameters(0.1) ; #print module.get_parameters()  \n",
      "\n",
      "plot_frontiere(data.x,module.forward)\n",
      "plot2DSet(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Logistique"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# In[7]:\n",
      "\n",
      "class TanHModule( Module ) :\n",
      "    \n",
      "    def __init__(self,dimension):\n",
      "        self.dimension = dimension\n",
      "    \n",
      "    #Permet le calcul de la sortie du module\n",
      "    def forward(self,input):\n",
      "        return np.tanh(input)\n",
      "    \n",
      "    #Permet le calcul du gradient des cellules d'entr\u00e9e\n",
      "    def backward_delta(self,input,delta_module_suivant):\n",
      "        \n",
      "        delta_in = np.dot(input,delta_module_suivant)\n",
      "        return delta_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data=createGaussianDataset(-1,-1,1,1,1,1,100)\n",
      "#print np.hstack((set.x,set.y))\n",
      "#data.x = kernel_poly(data.x)\n",
      "label = data.y\n",
      "#plot2DSet(set)\n",
      "\n",
      "hinge = HingeLoss()\n",
      "square = SquareLoss()\n",
      "\n",
      "m1 = LinearModule(data.x.shape[1],20)\n",
      "m2 = TanHModule(20)\n",
      "m3 = LinearModule(20,1)\n",
      "\n",
      "\n",
      "\n",
      "m1.randomize_parameters(1)\n",
      "m2.randomize_parameters(1)\n",
      "\n",
      "for i in range(200):\n",
      "    \n",
      "    \n",
      "    t =np.random.choice(data.x.shape[0])\n",
      "    x = data.x[t,:]\n",
      "    y = label[t]\n",
      "    \n",
      "    m1.init_gradient()\n",
      "    m2.init_gradient()\n",
      "\n",
      "    \n",
      "    m1_out = m1.forward(x) ; \n",
      "    \n",
      "    m2_out = m2.forward(m1_out)\n",
      "    \n",
      "    m3_out = m3.forward(m2_out)\n",
      "    \n",
      "    \n",
      "    delta_sortie_m3 = square.backward(m3_out,y) ; #print delta_sortie_m2\n",
      "    \n",
      "    m3.backward_update_gradient(m2_out,delta_sortie_m3)\n",
      "    delta_sortie_m2 = m3.backward_delta(m2_out,delta_sortie_m3) ; #print \"delta_sortie_m1\" , delta_sortie_m1\n",
      "    \n",
      "    m2.backward_update_gradient(m1_out,delta_sortie_m2)\n",
      "    delta_sortie_m1 = m2.backward_delta(m1_out,delta_sortie_m2)\n",
      "    \n",
      "    m1.backward_update_gradient(x,delta_sortie_m1)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    m1.update_parameters(0.01)\n",
      "    m2.update_parameters(0.01)\n",
      "    m3.update_parameters(0.01)\n",
      "    \n",
      "#print m1.get_parameters().shape\n",
      "#print m2.get_parameters()\n",
      "#plot_frontiere(data.x,module.forward)\n",
      "plot_frontiere(data.x,module.forward)\n",
      "plot2DSet(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test du Syst\u00e8me\n",
      "======\n",
      "\n",
      "Nous allons maintenant tester notre syst\u00e8me (Module Lin\u00e9aire-> Square Loss) sur un jeu de donn\u00e9es classiques (jeu en 2D du TP pr\u00e9c\u00e9dent). Est-ce que ca marche ? \n",
      "Essayez maintenant avec un hinge loss. Est-ce que ca marche?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Modules Additionnels\n",
      "======\n",
      "\n",
      "Nous allons impl\u00e9menter les modules suivants:\n",
      "* Module Tangente Hyperbolic\n",
      "* Module S\u00e9quentiel\n",
      "\n",
      "Nous pouvons maintenant faire des r\u00e9seaux de neurones ! "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}