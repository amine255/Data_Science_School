{
 "metadata": {
  "name": "",
  "signature": "sha256:0f79d239be0eed1d9b158044e260c0ad07e264713d7e7bf8edaccd121d2e326e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import gensim      \n",
      "from gensim import corpora\n",
      "from tools import *\n",
      "import os.path\n",
      "import sklearn.feature_extraction.text as txtTools #.TfidfTransformes\n",
      "import codecs\n",
      "import re\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = set('a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your'.split(\",\"))\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data_from_folder(path):\n",
      "    \n",
      "    alltxts = [] # init vide\n",
      "    labs = []\n",
      "    cpt = -1\n",
      "    for cl in os.listdir(path): # parcours des fichiers d'un r\u00e9pertoire\n",
      "        print cl\n",
      "        for f in os.listdir(path+cl):\n",
      "            txt = readAFile(path+cl+'/'+f)\n",
      "            alltxts.append(txt)\n",
      "            labs.append(cpt)\n",
      "\n",
      "\n",
      "        cpt += 2\n",
      "    return alltxts, labs\n",
      "\n",
      "\n",
      "\n",
      "def create_dictionary(alltxts,labs):\n",
      "    print \"Creating Bag of Words...\"  \n",
      "    \n",
      "    \n",
      "    stopword_path = \"/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/stopwords/stop-words_english_1_en.txt\"\n",
      "\n",
      "    #stoplist = set('le la les de des \u00e0 un une en au ne ce d l c s je tu il que qui mais quand'.split())\n",
      "    stoplist = set('a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your'.split(\",\"))\n",
      "    #stoplist = set(get_test_data(stopword_path,\"\\r\\n\"))\n",
      "\n",
      "    stoplist.add('')\n",
      "\n",
      "    ## DICO\n",
      "    splitters = u'; |, |\\*|\\. | |\\'|'\n",
      "\n",
      "    dictionary = corpora.Dictionary(re.split(splitters, doc.lower()) for doc in alltxts)\n",
      "\n",
      "    print len(dictionary)\n",
      "\n",
      "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]\n",
      "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 10]\n",
      "    dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "    dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "\n",
      "    print len(dictionary)\n",
      "\n",
      "    return stoplist,splitters,dictionary\n",
      "\n",
      "\n",
      "def create_corpus(stoplist,splitters,dictionary,alltxts):\n",
      "    \n",
      "    texts = [[word for word in re.split(splitters, document.lower()) if word not in stoplist]  for document in alltxts]\n",
      "    \n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "    ## exemple de doc\n",
      "    # corpus[0]\n",
      "    # avec les mots\n",
      "    #print [dictionary[i] for i,tmp in corpus[0]]\n",
      "\n",
      "    # Transformation pour passer en matrice numpy\n",
      "    dataSparse = gensim.matutils.corpus2csc(corpus)\n",
      "    dataSparse = dataSparse.T\n",
      "\n",
      "        ### tfidf\n",
      "    t = txtTools.TfidfTransformer()\n",
      "    t.fit(dataSparse.T)\n",
      "    data2 = t.transform(dataSparse.T)\n",
      "    data2 = data2.T\n",
      "\n",
      "    return dataSparse,data2\n",
      "    \n",
      "    \n",
      "def get_test_data(path,delimiter):\n",
      "    txt = readAFile(path)\n",
      "    return txt.split(delimiter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#IMPORTING DATA\n",
      "\n",
      "path_train = \"/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/trainset/\"\n",
      "path_test = \"/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/testset/test/testSentiment.txt\"\n",
      "\n",
      "\n",
      "\n",
      "alltxts_train,labs = get_data_from_folder(path_train)\n",
      "alltxts_test= get_test_data(path_test,\"\\n\")\n",
      "\n",
      "\n",
      "stoplist,splitters,dictionary = create_dictionary(alltxts_train,labs)\n",
      "dataSparse_train,data2_train = create_corpus(stoplist,splitters,dictionary,alltxts_train)\n",
      "dataSparse_test,data2_test = create_corpus(stoplist,splitters,dictionary,alltxts_test)\n",
      "\n",
      "\n",
      "\n",
      "print \"data2_train.shape :\", data2_train.shape\n",
      "print \"data2_train.shape :\", data2_train.shape\n",
      "\n",
      "print \"data2_test.shape :\", data2_test.shape\n",
      "print \"data2_test.shape :\", data2_test.shape\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "neg\n",
        "pos\n",
        "Creating Bag of Words..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47169"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7991\n",
        "data2_train.shape :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (2000, 7991)\n",
        "data2_train.shape : (2000, 7991)\n",
        "data2_test.shape : (25001, 7991)\n",
        "data2_test.shape : (25001, 7991)\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#DEFINE CLASSIFIER\n",
      "\n",
      "\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import linear_model\n",
      "from sklearn import svm\n",
      "from sklearn import tree\n",
      "\n",
      "\n",
      "clf = svm.SVC(C=1)  # definition du classifieur\n",
      "#clf = MultinomialNB(alpha=1)\n",
      "#clf = linear_model.SGDClassifier()\n",
      "\n",
      "\n",
      "print \"Training parameters\"\n",
      "clf.fit(dataSparse_train,labs)   # apprentissage"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training parameters\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": [
        "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#EVALUATE CROSS VALIDATION\n",
      "\n",
      "#from sklearn import svm\n",
      "from sklearn import cross_validation\n",
      "\n",
      "## usage basique: split train/test\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split( dataSparse_train, labs, test_size=0.4, random_state=0)\n",
      "\n",
      "scores = cross_validation.cross_val_score( clf, dataSparse_train, labs, cv=5)\n",
      "print scores\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.7225  0.765   0.7375  0.6875  0.7375]\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#PREDICTION\n",
      "prediction = clf.predict(dataSparse_test)\n",
      "print prediction.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(25001,)\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#WRITING TO TEXTFILE\n",
      "\n",
      "text_file = open(\"prediction\", \"w\")\n",
      "for i in range(0,prediction.shape[0]-1):\n",
      "    if prediction[i] == -1.0:\n",
      "        text_file.write(\"C\\n\")\n",
      "    else:\n",
      "        text_file.write(\"M\\n\")\n",
      "            \n",
      "text_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}