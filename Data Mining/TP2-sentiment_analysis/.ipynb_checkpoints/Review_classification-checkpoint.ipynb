{
 "metadata": {
  "name": "",
  "signature": "sha256:fb81e4c4aaa1cc95cc812e87f0066a86d0b66b544c49e1947e3c43963569efed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import gensim      \n",
      "from gensim import corpora\n",
      "from tools import *\n",
      "import os.path\n",
      "import sklearn.feature_extraction.text as txtTools #.TfidfTransformes\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(path):\n",
      "    \n",
      "    alltxts = [] # init vide\n",
      "    labs = []\n",
      "    cpt = 0\n",
      "    for cl in os.listdir(path): # parcours des fichiers d'un r\u00e9pertoire\n",
      "        print cl\n",
      "        for f in os.listdir(path+cl):\n",
      "            txt = readAFile(path+cl+'/'+f)\n",
      "            alltxts.append(txt)\n",
      "            labs.append(cpt)\n",
      "\n",
      "\n",
      "        cpt += 1\n",
      "    return alltxts, labs\n",
      "\n",
      "\n",
      "\n",
      "def create_BoW(alltxts,labs):\n",
      "    print \"Creating Bag of Words...\"  \n",
      "    \n",
      "\n",
      "    stoplist = set('le la les de des \u00e0 un une en au ne ce d l c s je tu il que qui mais quand'.split())\n",
      "    stoplist = set(''.split())\n",
      "    stoplist.add('')\n",
      "\n",
      "    ## DICO\n",
      "    splitters = u'; |, |\\*|\\. | |\\'|'\n",
      "\n",
      "    dictionary = corpora.Dictionary(re.split(splitters, doc.lower()) for doc in alltxts)\n",
      "\n",
      "    print len(dictionary)\n",
      "\n",
      "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]\n",
      "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 10]\n",
      "    dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "    dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "\n",
      "    print len(dictionary)\n",
      "\n",
      "    ## PROJ\n",
      "\n",
      "    texts = [[word for word in re.split(splitters, document.lower()) if word not in stoplist]  for document in alltxts]\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "    ## exemple de doc\n",
      "    # corpus[0]\n",
      "    # avec les mots\n",
      "    print [dictionary[i] for i,tmp in corpus[0]]\n",
      "\n",
      "    # Transformation pour passer en matrice numpy\n",
      "    dataSparse = gensim.matutils.corpus2csc(corpus)\n",
      "    dataSparse = dataSparse.T\n",
      "\n",
      "    ### tfidf\n",
      "    t = txtTools.TfidfTransformer()\n",
      "    t.fit(dataSparse.T)\n",
      "    data2 = t.transform(dataSparse.T)\n",
      "    data2 = data2.T\n",
      "    \n",
      "    return dataSparse,data2\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = \"/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/\"\n",
      "\n",
      "alltxts,labs = get_data(path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "neg\n",
        "pos"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################       \n",
      "#BAG OF WORDS\n",
      "####################################################\n",
      "print \"Creating Bag of Words...\"  \n",
      "import gensim      \n",
      "from gensim import corpora\n",
      "      \n",
      "stoplist = set('le la les de des \u00e0 un une en au ne ce d l c s je tu il que qui mais quand'.split())\n",
      "stoplist = set(''.split())\n",
      "stoplist.add('')\n",
      "\n",
      "## DICO\n",
      "splitters = u'; |, |\\*|\\. | |\\'|'\n",
      "\n",
      "dictionary = corpora.Dictionary(re.split(splitters, doc.lower()) for doc in alltxts)\n",
      "\n",
      "print len(dictionary)\n",
      "\n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 10]\n",
      "dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "\n",
      "print len(dictionary)\n",
      "\n",
      "## PROJ\n",
      "\n",
      "texts = [[word for word in re.split(splitters, document.lower()) if word not in stoplist]  for document in alltxts]\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "## exemple de doc\n",
      "# corpus[0]\n",
      "# avec les mots\n",
      "print [dictionary[i] for i,tmp in corpus[0]]\n",
      "\n",
      "# Transformation pour passer en matrice numpy\n",
      "dataSparse = gensim.matutils.corpus2csc(corpus)\n",
      "dataSparse = dataSparse.T\n",
      "                                                                                                                                                                                                                        \n",
      "### tfidf\n",
      "import sklearn.feature_extraction.text as txtTools #.TfidfTransformes\n",
      "#\n",
      "t = txtTools.TfidfTransformer()\n",
      "t.fit(dataSparse.T)\n",
      "data2 = t.transform(dataSparse.T)\n",
      "data2 = data2.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating Bag of Words...\n",
        "47169"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'hanging', u'stereotypical', u'here', u'how', u're', u'than', u'rich', u'best', u'much', u'played', u'has', u'right', u'for', u'bottom', u'happy', u'time', u'love', u'angry', u'by', u'films', u'spot', u'such', u'so', u'wrong', u'one', u'alley', u'allen', u'screen', u's', u'power', u'her', u'hey', u'pull', u'an', u'as', u'at', u'tv', u'to', u'smile', u'hours', u'more', u'entire', u'seinfeld', u'imagine', u'number', u'flimsy', u'marriage', u'also', u'wayne', u'movie', u'community', u'great', u'funny', u'couple', u'be', u'into', u':', u'up', u'us', u'actors', u'trite', u'let', u'popcorn', u'makes', u'this', u'newman', u'could', u'fact', u'brink', u'should', u'figures', u've', u'moments', u'unwatchable', u'total', u'insult', u'1997', u'farce', u'then', u'they', u'what', u'notion', u'do', u'away', u'we', u'brad', u'the', u'thanks', u'way', u'was', u'reality', u'convincing', u'seemingly', u'still', u'not', u'now', u'nor', u'directing', u'year', u'that', u'ground', u'only', u'namely', u'3', u'between', u't', u'you', u'since', u'very', u'respect', u'answered', u'screenwriters', u'a', u'help', u'hell', u'miss', u'ill-conceived', u'my', u'pretty', u'catches', u'probably', u'else', u'look', u'popular', u'some', u'run', u'doesn', u'final', u'!', u'(', u'horribly', u'i', u'?', u'refuge', u'previously', u'accountant', u'likely', u'even', u'while', u'give', u'laughs', u'navy', u'grade', u'seem', u'seek', u'm', u'points', u'bad', u'seven', u'is', u'it', u'in', u'if', u'just', u'bob', u'making', u'correct', u'after', u'thing', u'first', u'sounds', u'were', u'pain', u'get', u'sound', u'them', u'totally', u'\\n', u'film', u'steve', u'two', u'needs', u'through', u'suffer', u'noticing', u'everyone', u'idea', u'engaging', u'beginning', u'b', u'chemistry', u'big', u'bit', u'absolutely', u'back', u'either', u'directed', u'knight', u'talent', u'names', u'll', u'nobody', u'have', u')', u'faces', u'like', u'about', u'but', u'minutes', u'join', u'end', u'badly', u'over', u'got', u'too', u'discovers', u'been', u'everything', u'believable', u'no', u'tim', u'scripted', u'r', u'and', u'cast', u'someone', u'moronic', u'seems', u'except', u'on', u'ok', u'of', u'or', u'there', u'cinema', u'includes', u'all', u'worst', u'list', u'ten', u'sum', u'trouble', u'thread', u'\"', u'script', u'nothing', u'notch', u'his', u'cringe', u'are', u'duo', u'deals', u'from', u'six', u'looks', u'bumbling', u'their', u'boring', u'which', u'who', u'ensues', u'ones', u'married', u'unconvincing', u'decent', u'divorce', u'will', u'gets', u'half', u'know', u'made']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}