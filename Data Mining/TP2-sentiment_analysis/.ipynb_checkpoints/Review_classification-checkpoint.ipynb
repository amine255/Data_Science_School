{
 "metadata": {
  "name": "",
  "signature": "sha256:2668e3e59b96cc29a5d69979483f7c6f9ce0e872ac89512d9ccf05e8485ec58f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import gensim      \n",
      "from gensim import corpora\n",
      "from tools import *\n",
      "import os.path\n",
      "import sklearn.feature_extraction.text as txtTools #.TfidfTransformes\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(path):\n",
      "    \n",
      "    alltxts = [] # init vide\n",
      "    labs = []\n",
      "    cpt = 0\n",
      "    for cl in os.listdir(path): # parcours des fichiers d'un r\u00e9pertoire\n",
      "        print cl\n",
      "        for f in os.listdir(path+cl):\n",
      "            txt = readAFile(path+cl+'/'+f)\n",
      "            alltxts.append(txt)\n",
      "            labs.append(cpt)\n",
      "\n",
      "\n",
      "        cpt += 1\n",
      "    return alltxts, labs\n",
      "\n",
      "\n",
      "\n",
      "def create_BoW(alltxts,labs):\n",
      "    print \"Creating Bag of Words...\"  \n",
      "    \n",
      "\n",
      "    stoplist = set('le la les de des \u00e0 un une en au ne ce d l c s je tu il que qui mais quand'.split())\n",
      "    stoplist = set(''.split())\n",
      "    stoplist.add('')\n",
      "\n",
      "    ## DICO\n",
      "    splitters = u'; |, |\\*|\\. | |\\'|'\n",
      "\n",
      "    dictionary = corpora.Dictionary(re.split(splitters, doc.lower()) for doc in alltxts)\n",
      "\n",
      "    print len(dictionary)\n",
      "\n",
      "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]\n",
      "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 10]\n",
      "    dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "    dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "\n",
      "    print len(dictionary)\n",
      "\n",
      "    ## PROJ\n",
      "\n",
      "    texts = [[word for word in re.split(splitters, document.lower()) if word not in stoplist]  for document in alltxts]\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "    ## exemple de doc\n",
      "    # corpus[0]\n",
      "    # avec les mots\n",
      "    #print [dictionary[i] for i,tmp in corpus[0]]\n",
      "\n",
      "    # Transformation pour passer en matrice numpy\n",
      "    dataSparse = gensim.matutils.corpus2csc(corpus)\n",
      "    dataSparse = dataSparse.T\n",
      "\n",
      "    ### tfidf\n",
      "    t = txtTools.TfidfTransformer()\n",
      "    t.fit(dataSparse.T)\n",
      "    data2 = t.transform(dataSparse.T)\n",
      "    data2 = data2.T\n",
      "    \n",
      "    return dataSparse,data2\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = \"/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/trainset/\"\n",
      "path1 = \"/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/\"\n",
      "\n",
      "\n",
      "#alltxts,labs = get_data(path)\n",
      "alltxts,labs = get_data(path1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trainset\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "[Errno 21] Is a directory: '/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/trainset/neg'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-25-5f271f4e5405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#alltxts,labs = get_data(path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0malltxts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-16-ce83e8d537d7>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadAFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0malltxts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mlabs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/tools.pyc\u001b[0m in \u001b[0;36mreadAFile\u001b[1;34m(nf)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreadAFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 21] Is a directory: '/home/arda/Documents/Data_Science_School/Data Mining/TP2-sentiment_analysis/data/trainset/neg'"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataSparse,data2 = create_BoW(alltxts,labs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating Bag of Words...\n",
        "47169"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataSparse.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(2000, 8108)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "#DEFINE CLASSIFIER\n",
      "#########################################################\n",
      "print \"Defining classifier\"\n",
      "\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import linear_model\n",
      "from sklearn import svm\n",
      "from sklearn import tree\n",
      "\n",
      "#X = [[0, 0], [1, 1]]\n",
      "#y = [0, 1]\n",
      "#clf = svm.LinearSVC(C=1)  # definition du classifieur\n",
      "clf = MultinomialNB(alpha=1)\n",
      "#clf = linear_model.SGDClassifier()\n",
      "\n",
      "\n",
      "\n",
      "print \"Training parameters\"\n",
      "clf.fit(dataSparse,labs)   # apprentissage"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Defining classifier\n",
        "Training parameters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "#EVALUATE CROSS VALIDATION\n",
      "#########################################################\n",
      "print \"Cross Validation...\"  \n",
      "#print \"Entering CF\"\n",
      "#\n",
      "#from sklearn import svm\n",
      "from sklearn import cross_validation\n",
      "\n",
      "## usage basique: split train/test\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split( dataSparse, labs, test_size=0.4, random_state=0)\n",
      "\n",
      "scores = cross_validation.cross_val_score( clf, dataSparse, labs, cv=5)\n",
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cross Validation...\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([ 0.8275,  0.85  ,  0.79  ,  0.8025,  0.825 ])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_file = open(\"Output.txt\", \"w\")\n",
      "for i in range(0,prediction.shape[0]):\n",
      "    if prediction[i] == 1.0:\n",
      "        text_file.write(\"C\\n\")\n",
      "    else:\n",
      "        text_file.write(\"M\\n\")\n",
      "            \n",
      "text_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}