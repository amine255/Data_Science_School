{
 "metadata": {
  "name": "",
  "signature": "sha256:de149abd18177e168a87e3d9136ab6746fdec37697f2a3a44a97df1d2e8ccea7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "ghj"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Created on Tue Sep 16 19:25:08 2014\n",
      "\n",
      "@author: arda\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from tools import *\n",
      "\n",
      "####################################################       \n",
      "#IMPORT DATA\n",
      "####################################################\n",
      "print \"Importing data...\"  \n",
      "corpus_train = \"Data/corpus.tache1.learn.utf8\"\n",
      "fname  = corpus_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Importing data...\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################       \n",
      "#DATA MUNGING\n",
      "####################################################\n",
      "print \"Munging data...\"  \n",
      "import codecs\n",
      "import re\n",
      "    \n",
      "nblignes = compteLignes(fname)\n",
      "print \"nblignes = %d\"%nblignes\n",
      "\n",
      "alltxts = []\n",
      "labs = np.ones(nblignes)\n",
      "s=codecs.open(fname, 'r','utf-8') # pour r\u00e9gler le codage\n",
      "\n",
      "cpt = 0\n",
      "for i in range(nblignes):\n",
      "    txt = s.readline()\n",
      "    #print txt\n",
      "\n",
      "    lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
      "    txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
      "\n",
      "    #assert(lab == \"C\" or lab == \"M\")\n",
      "\n",
      "    if lab.count('M') >0:\n",
      "        labs[cpt] = -1\n",
      "    alltxts.append(txt)\n",
      "\n",
      "    cpt += 1\n",
      "\n",
      "####################################################       \n",
      "#BAG OF WORDS\n",
      "####################################################\n",
      "print \"Creating Bag of Words...\"  \n",
      "import gensim      \n",
      "from gensim import corpora\n",
      "      \n",
      "#stoplist = set('le la les de des \u00e0 un une en au ne ce d l c s je tu il que qui mais quand'.split())\n",
      "stoplist = set(''.split())\n",
      "stoplist.add('')\n",
      "\n",
      "## DICO\n",
      "splitters = u'; |, |\\*|\\. | |\\'|'\n",
      "\n",
      "dictionary = corpora.Dictionary(re.split(splitters, doc.lower()) for doc in alltxts)\n",
      "\n",
      "print len(dictionary)\n",
      "\n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 1]\n",
      "dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "\n",
      "print len(dictionary)\n",
      "\n",
      "## PROJ\n",
      "\n",
      "texts = [[word for word in re.split(splitters, document.lower()) if word not in stoplist]  for document in alltxts]\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "## exemple de doc\n",
      "# corpus[0]\n",
      "# avec les mots\n",
      "print [dictionary[i] for i,tmp in corpus[0]]\n",
      "\n",
      "# Transformation pour passer en matrice numpy\n",
      "dataSparse = gensim.matutils.corpus2csc(corpus)\n",
      "dataSparse = dataSparse.T\n",
      "                                                                                                                                                                                                                        \n",
      "### tfidf\n",
      "import sklearn.feature_extraction.text as txtTools #.TfidfTransformes\n",
      "#\n",
      "t = txtTools.TfidfTransformer()\n",
      "t.fit(dataSparse.T)\n",
      "data2 = t.transform(dataSparse.T)\n",
      "data2 = data2.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Munging data...\n",
        "nblignes = 57413"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creating Bag of Words..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40195"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u's', u'l', u'que', u'amis', u'chers', u'd', u'expression', u'ce', u'de', u'agit', u'il', u'l\\xe0', u'ne', u'une', u'je', u'formule', u'quand', u'dis', u'mais', u'pas', u'ressens.\\n', u'diplomatique']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(alltxts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "57413\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dataSparse = data2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "#DEFINE CLASSIFIER\n",
      "#########################################################\n",
      "print \"Defining classifier\"\n",
      "\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import linear_model\n",
      "from sklearn import svm\n",
      "from sklearn import tree\n",
      "\n",
      "#X = [[0, 0], [1, 1]]\n",
      "#y = [0, 1]\n",
      "#clf = svm.LinearSVC(C=1)  # definition du classifieur\n",
      "clf = MultinomialNB(alpha=1)\n",
      "#clf = linear_model.SGDClassifier()\n",
      "\n",
      "\n",
      "\n",
      "print \"Training parameters\"\n",
      "clf.fit(dataSparse,labs)   # apprentissage"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Defining classifier\n",
        "Training parameters\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 308,
       "text": [
        "MultinomialNB(alpha=1, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 308
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "#EVALUATE CROSS VALIDATION\n",
      "#########################################################\n",
      "print \"Cross Validation...\"  \n",
      "#print \"Entering CF\"\n",
      "#\n",
      "#from sklearn import svm\n",
      "from sklearn import cross_validation\n",
      "\n",
      "## usage basique: split train/test\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split( dataSparse, labs, test_size=0.4, random_state=0)\n",
      "\n",
      "scores = cross_validation.cross_val_score( clf, dataSparse, labs, cv=5)\n",
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cross Validation...\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 309,
       "text": [
        "array([ 0.88992424,  0.88914047,  0.89410433,  0.88913081,  0.88599547])"
       ]
      }
     ],
     "prompt_number": 309
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################################################\n",
      "#PREDICTING\n",
      "#########################################################\n",
      "\n",
      "corpus_to_predict = \"Data/corpus.tache1.test.utf8\"\n",
      "fname =corpus_to_predict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 310
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################       \n",
      "#DATA MUNGING\n",
      "####################################################\n",
      "print \"Munging data...\"  \n",
      "import codecs\n",
      "import re\n",
      "    \n",
      "nblignes = compteLignes(fname)\n",
      "print \"nblignes = %d\"%nblignes\n",
      "\n",
      "alltxts = []\n",
      "labs = np.ones(nblignes)\n",
      "s=codecs.open(fname, 'r','utf-8') # pour r\u00e9gler le codage\n",
      "\n",
      "cpt = 0\n",
      "for i in range(nblignes):\n",
      "    txt = s.readline()\n",
      "    #print txt\n",
      "\n",
      "    lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
      "    txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
      "\n",
      "    #assert(lab == \"C\" or lab == \"M\")\n",
      "\n",
      "    if lab.count('M') >0:\n",
      "        labs[cpt] = -1\n",
      "    alltxts.append(txt)\n",
      "\n",
      "    cpt += 1\n",
      "\n",
      "####################################################       \n",
      "#BAG OF WORDS\n",
      "####################################################\n",
      "print \"Creating Bag of Words...\"  \n",
      "import gensim      \n",
      "from gensim import corpora\n",
      "      \n",
      "#stoplist = set('le la les de des \u00e0 un une en au ne ce d l c s je tu il que qui mais quand'.split())\n",
      "#stoplist.add('')\n",
      "\n",
      "## DICO\n",
      "#splitters = u'; |, |\\*|\\. | |\\'|'\n",
      "\n",
      "#dictionary = corpora.Dictionary(re.split(splitters, doc.lower()) for doc in alltxts)\n",
      "\n",
      "#print len(dictionary)\n",
      "\n",
      "#stop_ids = [dictionary.token2id[stopword] for stopword in stoplist   if stopword in dictionary.token2id]\n",
      "#once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 10 ]\n",
      "#dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
      "#dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
      "\n",
      "#print len(dictionary)\n",
      "\n",
      "## PROJ\n",
      "\n",
      "texts = [[word for word in re.split(splitters, document.lower()) if word not in stoplist]  for document in alltxts]\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "## exemple de doc\n",
      "# corpus[0]\n",
      "# avec les mots\n",
      "print [dictionary[i] for i,tmp in corpus[0]]\n",
      "\n",
      "# Transformation pour passer en matrice numpy\n",
      "dataSparse = gensim.matutils.corpus2csc(corpus)\n",
      "dataSparse = dataSparse.T\n",
      "                                                                                                                                                                                                                        \n",
      "### tfidf\n",
      "import sklearn.feature_extraction.text as txtTools #.TfidfTransformes\n",
      "#\n",
      "t = txtTools.TfidfTransformer()\n",
      "t.fit(dataSparse.T)\n",
      "data2 = t.transform(dataSparse.T)\n",
      "data2 = data2.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Munging data...\n",
        "nblignes = 27162\n",
        "Creating Bag of Words..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'cette', u'avec', u'confiance', u'l', u'amiti\\xe9', u'invitation', u'votre', u'histoire', u'depuis', u'r\\xe9pondant', u'nouveau', u'd', u'alg\\xe9rie', u'mutuel', u'ai', u'notre', u'de', u'du', u'et', u'conscience', u'visite', u'effectuant', u'le', u'ouvrir', u'respect', u'en', u'premi\\xe8re', u'un', u'la', u'chapitre', u'solidarit\\xe9.\\n', u'vous', u'\\xe0', u'commune', u'fran\\xe7ais', u'pr\\xe9sident', u'estime', u'j', u'etat', u'ind\\xe9pendance']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 311
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 314
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction =clf.predict(dataSparse)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "dimension mismatch",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-315-d23ac8178957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/usr/lib/pymodules/python2.7/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/pymodules/python2.7/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;34m\"\"\"Calculate the posterior log probability of the samples X\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast2d_or_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T)\n\u001b[0m\u001b[1;32m    366\u001b[0m                + self.class_log_prior_)\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/pymodules/python2.7/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
       ]
      }
     ],
     "prompt_number": 315
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 273,
       "text": [
        "array([ 1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
        "       -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
        "        1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
        "       -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
       ]
      }
     ],
     "prompt_number": 273
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(prediction.shape[0]-1):\n",
      "    if prediction[i] == -1:\n",
      "        prediction[i-1] = -1\n",
      "        prediction[i+1] = -1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 274,
       "text": [
        "array([ 1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
        "       -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
        "        1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
        "       -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
       ]
      }
     ],
     "prompt_number": 274
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_file = open(\"Output.txt\", \"w\")\n",
      "for i in range(0,prediction.shape[0]):\n",
      "    if prediction[i] == 1.0:\n",
      "        text_file.write(\"C\\n\")\n",
      "    else:\n",
      "        text_file.write(\"M\\n\")\n",
      "            \n",
      "text_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 275
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_file = open(\"Output.txt\", \"rw\")\n",
      "\n",
      "alllines = text_file.readlines()\n",
      "for i, line in enumerate(alllines):\n",
      "    if line.startswith(\"M\"):   \n",
      "        line.replace('M','XDLOL')\n",
      "        \n",
      "text_file.close()\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}