{
 "metadata": {
  "name": "TP3-recommander-sys",
  "signature": "sha256:de7d8bb0a5fc7267feb98edcbeee294ea4b5238977a661956725e8ccff370f30"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.linalg as lin\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.sparse import coo_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def import_data_tain_test(train_db, test_db):\n",
      "    # u.data  user id | item id | rating | timestamp.\n",
      "    data_train = np.loadtxt(train_db)\n",
      "    data_test = np.loadtxt(test_db)\n",
      "    return data_train, data_test\n",
      "\n",
      "\n",
      "def bias_calculus(data):\n",
      "    maxs = data.max(0)  # extraction des nb d'utilisateurs et d'item\n",
      "    nu = maxs[0] + 1\n",
      "    ni = maxs[1] + 1\n",
      "    user_bias = np.zeros(nu)\n",
      "    item_bias = np.zeros(ni)\n",
      "    user_count = np.zeros(nu)\n",
      "    item_count = np.zeros(ni)\n",
      "    # systeme basique: plus simple de travailler sur les triplets\n",
      "    for iteration in xrange(len(data)):\n",
      "        u = data[iteration, 0]\n",
      "        i = data[iteration, 1]\n",
      "        r = data[iteration, 2]\n",
      "        user_bias[u] += r\n",
      "        item_bias[i] += r\n",
      "        user_count[u] += 1\n",
      "        item_count[i] += 1\n",
      "    \n",
      "    # ATTENTION AUX DIVISIONS PAR 0 !!\n",
      "    user_bias /= np.where(user_count == 0, 1, user_count)\n",
      "    item_bias /= np.where(item_count == 0, 1, item_count)\n",
      "    print u\"user and item biases computed.\"    \n",
      "\n",
      "\n",
      "    return user_bias,item_bias\n",
      "\n",
      "\n",
      "def plot_distribution(user_bias,item_bias):\n",
      "    \n",
      "    plt.figure()\n",
      "    plt.subplot(211)\n",
      "    plt.hist(user_bias,100)\n",
      "    plt.title('Utilisateur')    \n",
      "    plt.subplot(212)\n",
      "    plt.hist(item_bias,100)\n",
      "    plt.title('Item')\n",
      "    plt.savefig(\"recoBias.pdf\")\n",
      "\n",
      "\n",
      "def matrix_facotisation(data):\n",
      "    # factorisation matricielle: on travaille de nouveau sur les triplets pour minimiser les hypoth\u00e8ses\n",
      "    # sur les cases vides\n",
      "    maxs = data.max(0)  # extraction des nb d'utilisateurs et d'item\n",
      "    nu = maxs[0] + 1\n",
      "    ni = maxs[1] + 1\n",
      "\n",
      "    # param\u00e9trage\n",
      "    random = np.random.RandomState(0)\n",
      "    epochs = 5 # nb de passage sur la base\n",
      "    nZ = 10 # taille de l'espace latent\n",
      "    l1_weight = 0.00 # contraintes de r\u00e9gularization L1 + L2\n",
      "    l2_weight = 0.0001\n",
      "    learning_rate = 0.001\n",
      "\n",
      "    train_indexes = np.arange(len(data)) # pour cet exemple, je prends tous les indices en apprentissage...\n",
      "\n",
      "    # initialisation \u00e0 moiti\u00e9 vide... randn + seuillage > 0\n",
      "    user_latent = np.random.randn(nu, nZ)\n",
      "    item_latent = np.random.randn(ni, nZ)\n",
      "    user_latent = np.where(user_latent > 0, user_latent, 0) # profils positifs sparses\n",
      "    item_latent = np.where(item_latent > 0, item_latent, 0)\n",
      "\n",
      "    for epoch in xrange(epochs):\n",
      "        print \"epoch : %d\"%epoch\n",
      "        # Update\n",
      "        random.shuffle(train_indexes)\n",
      "        for index in train_indexes:\n",
      "            # extraction des variables => lisibilit\u00e9\n",
      "            label, user, item = data[index,2], data[index,0], data[index,1]\n",
      "            gamma_u, gamma_i = user_latent[user, :], item_latent[item, :]\n",
      "            # Optimisation\n",
      "            delta_label = 2 * (label - np.dot(gamma_u, gamma_i))\n",
      "            gradient_u = l2_weight * gamma_u + l1_weight - delta_label * gamma_i\n",
      "            gamma_u_prime = gamma_u - learning_rate * gradient_u\n",
      "            user_latent[user, :] = np.where(gamma_u_prime * gamma_u > 0, gamma_u_prime, 0) # MAJ user\n",
      "            gradient_i = l2_weight * gamma_i + l1_weight - delta_label * gamma_u\n",
      "            gamma_i_prime = gamma_i - learning_rate * gradient_i\n",
      "            item_latent[item, :] = np.where(gamma_i_prime * gamma_i > 0, gamma_i_prime, 0) # MAJ item\n",
      "    return user_latent, item_latent\n",
      "\n",
      "\n",
      "def user_visualisation(user_latent):\n",
      "    # visualisation des users\n",
      "    plt.figure()\n",
      "    plt.imshow(user_latent[:100,:], interpolation=\"nearest\") # 100 premiers utilisateurs\n",
      "    plt.colorbar()\n",
      "    #plt.savefig(\"userLatent.pdf\")\n",
      "    \n",
      "def item_visualisation(item_latent):\n",
      "    # visualisation des users\n",
      "    code,sig,dico = lin.svd(item_latent)\n",
      "    item2d = code[:,:2] # deux premieres colonnes = 2 premi\u00e8res valeurs singuli\u00e8res (les plus fortes)\n",
      "\n",
      "    plt.figure()\n",
      "    plt.scatter(item2d[:,0], item2d[:,1])\n",
      "    #plt.savefig(\"itemLatent.pdf\")\n",
      "    \n",
      "def Sparse(data):\n",
      "    nu = max(data[:,0]) ; #print \"nu :\", nu\n",
      "    ni = max(data[:,1]) ; #print \"ni :\", ni\n",
      "    row = data[:,0]\n",
      "    col = data[:,1] \n",
      "    rat = data[:,2]\n",
      "    A  = coo_matrix((rat,(row,col))).todense()\n",
      "    return A[1:,1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import data \n",
      "#train_db = \"/home/arda-mint/Documents/M2/Data Mining/TP3-Recommender-sys/data/u1.base\"\n",
      "train_db =\"/users/Etu6/3402426/Documents/M2/Data Mining/TP3-Recommender-sys/data/u1.base\"\n",
      "#train_db =\"/users/Etu6/3402426/Documents/M2/Data Mining/TP3-Recommender-sys/data/ub.base\"\n",
      "\n",
      "#test_db = \"/home/arda-mint/Documents/M2/Data Mining/TP3-Recommender-sys/data/u1.test\"\n",
      "test_db = \"/users/Etu6/3402426/Documents/M2/Data Mining/TP3-Recommender-sys/data/u1.test\"\n",
      "#test_db = \"/users/Etu6/3402426/Documents/M2/Data Mining/TP3-Recommender-sys/data/ub.test\"\n",
      "\n",
      "data_train = np.loadtxt(train_db)\n",
      "data_test = np.loadtxt(test_db)\n",
      "ub,ib = bias_calculus(data_train)\n",
      "print ub.shape, ib.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "user and item biases computed.\n",
        "(944,) (1683,)\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Calcul Mean Square Error by using user-bias (ub) & item-bias\n",
      "\n",
      "# u.data  user id | item id | rating | timestamp.\n",
      "count = 0\n",
      "diff = 0\n",
      "for i in range(data_test.shape[0]):\n",
      "    user = data_test[i,0]\n",
      "    diff += np.square(ub[user] - data_test[i,2])\n",
      "    count += 1\n",
      "MSE_ub = diff/count\n",
      "\n",
      "count = 0\n",
      "diff = 0\n",
      "for i in range(data_test.shape[0]):\n",
      "    item = data_test[i,1]\n",
      "    diff += np.square(ib[item] - data_test[i,2])\n",
      "    count += 1\n",
      "MSE_ib = diff/count\n",
      "\n",
      "print MSE_ub; print MSE_ib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.12995864142\n",
        "1.07342437661\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urs = Sparse(data_train)\n",
      "urs[0,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "matrix([[ 5.,  3.,  4., ...,  0.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "row  = np.array([0,3,1,0])\n",
      "col  = np.array([0,3,1,2])\n",
      "data = np.array([4,5,7,9])\n",
      "A = coo_matrix((data,(row,col)), shape=(4,4)).todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "matrix([[4, 0, 9, 0],\n",
        "        [0, 7, 0, 0],\n",
        "        [0, 0, 0, 0],\n",
        "        [0, 0, 0, 5]])"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "line = A[0,:]\n",
      "pos = line[line!=0]\n",
      "mean_line = pos.sum()/pos.shape[1]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}